{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "clinical = {}\n",
    "file_clinical = open('../liver_clinical.csv')\n",
    "clinical_ = csv.reader(file_clinical)\n",
    "for index,row in enumerate(clinical_):\n",
    "    if index == 0:\n",
    "        continue\n",
    "    clinical[row[0]] = row[1:]\n",
    "\n",
    "\n",
    "\n",
    "train_new = []\n",
    "file_train = open(\"./csv/validation.csv\")\n",
    "csv_f = csv.reader(file_train)\n",
    "\n",
    "\n",
    "for index,row in enumerate(csv_f):\n",
    "    if index == 0:\n",
    "        continue\n",
    "    pid = row[0]\n",
    "    tmp_feature = []\n",
    "    tmp_feature.extend(row[0:-3])\n",
    "    tmp_feature.extend(clinical[pid])\n",
    "    tmp_feature.extend(row[-1])\n",
    "    train_new.append(tmp_feature)\n",
    "data = pd.DataFrame(np.array(train_new))\n",
    "data.to_csv('./csv/validation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load features100.py\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tpot import TPOTClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score,auc, precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import hamming_loss\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "import joblib\n",
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import model_selection\n",
    "from tpot import TPOTClassifier\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "#from pyearth import Earth\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import f_classif\n",
    "from skfeature.function.statistical_based import t_score\n",
    "from skfeature.function.statistical_based import gini_index\n",
    "from skfeature.function.similarity_based import fisher_score\n",
    "from skfeature.function.similarity_based import reliefF\n",
    "\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from skfeature.function.information_theoretical_based import LCSI\n",
    "from skfeature.function.information_theoretical_based import MIM\n",
    "from skfeature.function.information_theoretical_based import MIFS\n",
    "from skfeature.function.information_theoretical_based import MRMR\n",
    "from skfeature.function.information_theoretical_based import CIFE\n",
    "from skfeature.function.information_theoretical_based import JMI\n",
    "from skfeature.function.information_theoretical_based import CMIM\n",
    "from skfeature.function.information_theoretical_based import ICAP\n",
    "from skfeature.function.information_theoretical_based import DISR\n",
    "\n",
    "from scipy.stats import wilcoxon\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.utils import resample\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import hamming_loss\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import warnings\n",
    "\n",
    "\n",
    "file_train = open(\"./csv/train_new.csv\")\n",
    "csv_f = csv.reader(file_train)\n",
    "features = next(csv_f)\n",
    "dataset = pd.read_csv(\"./csv/train_new.csv\", names=features, usecols=range(1,6132), dtype=np.float64, skiprows=1, low_memory=False)\n",
    "# INITIALIZING, CLEANING, AND STRATIFYING DATASET\n",
    "dataset[\"6131\"] = pd.to_numeric(dataset[\"6131\"], errors='coerce')\n",
    "dataset.dropna(axis=1, thresh=2, inplace=True)\n",
    "#dataset.dropna(how='all',thresh = 20,inplace=True)\n",
    "train_feature = np.array(dataset)\n",
    "wh_inf = np.isinf(train_feature)\n",
    "train_feature[wh_inf]=0\n",
    "wh_nan = np.isnan(train_feature)\n",
    "train_feature[wh_nan]=0\n",
    "\n",
    "file_validate = open(\"./csv/validation_new.csv\")\n",
    "csv_f = csv.reader(file_validate)\n",
    "features = next(csv_f)\n",
    "dataset = pd.read_csv(\"./csv/validation_new.csv\", names=features, usecols=range(1,6132), dtype=np.float64, skiprows=1, low_memory=False)\n",
    "# INITIALIZING, CLEANING, AND STRATIFYING DATASET\n",
    "dataset[\"6131\"] = pd.to_numeric(dataset[\"6131\"], errors='coerce')\n",
    "#dataset.dropna(axis=1, thresh=2, inplace=True)\n",
    "validate_feature = np.array(dataset)\n",
    "wh_inf = np.isinf(validate_feature)\n",
    "validate_feature[wh_inf]=0\n",
    "wh_nan = np.isnan(validate_feature)\n",
    "validate_feature[wh_nan]=0\n",
    "\n",
    "\n",
    "file_test = open(\"./csv/test_new.csv\")\n",
    "csv_f = csv.reader(file_test)\n",
    "features = next(csv_f)\n",
    "dataset = pd.read_csv(\"./csv/test_new.csv\", names=features, usecols=range(1,6132), dtype=np.float64, skiprows=1, low_memory=False)\n",
    "# INITIALIZING, CLEANING, AND STRATIFYING DATASET\n",
    "dataset[\"6131\"] = pd.to_numeric(dataset[\"6131\"], errors='coerce')\n",
    "#dataset.dropna(axis=1, thresh=2, inplace=True)\n",
    "test_feature = np.array(dataset)\n",
    "wh_inf = np.isinf(test_feature)\n",
    "test_feature[wh_inf]=0\n",
    "wh_nan = np.isnan(test_feature)\n",
    "test_feature[wh_nan]=0\n",
    "\n",
    "\n",
    "#only use image features\n",
    "X_train = train_feature[:,:6130]\n",
    "Y_train = train_feature[:,6130]\n",
    "Y_train = Y_train.astype('int32')\n",
    "\n",
    "X_validate = validate_feature[:,:6130]\n",
    "Y_validate = validate_feature[:,6130]\n",
    "Y_validate = Y_validate.astype('int32')\n",
    "\n",
    "X_test = test_feature[:,:6130]\n",
    "Y_test = test_feature[:,6130]\n",
    "Y_test = Y_test.astype('int32')\n",
    "seed = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "343\n",
      "145\n",
      "198\n",
      "97\n",
      "41\n",
      "56\n",
      "48\n",
      "20\n",
      "28\n"
     ]
    }
   ],
   "source": [
    "print(len(Y_train))\n",
    "print(len(Y_train[Y_train==1]))\n",
    "print(len(Y_train[Y_train==0]))\n",
    "print(len(Y_validate))\n",
    "print(len(Y_validate[Y_validate==1]))\n",
    "print(len(Y_validate[Y_validate==0]))\n",
    "print(len(Y_test))\n",
    "print(len(Y_test[Y_test==1]))\n",
    "print(len(Y_test[Y_test==0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
